{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2737a06b-8344-4f1c-98b9-252e2d403700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì imbalanced-learn library available\n",
      "üîí CREDIT CARD FRAUD DETECTION SYSTEM\n",
      "============================================================\n",
      "Technologies: Python, scikit-learn, imbalanced-learn\n",
      "Dataset: Synthetic (replace with real creditcard.csv)\n",
      "============================================================\n",
      "Creating synthetic fraud detection dataset...\n",
      "Generating 50,000 transactions:\n",
      "  Normal: 49,900 (99.8%)\n",
      "  Fraud: 100 (0.200%)\n",
      "\n",
      "============================================================\n",
      "DATA EXPLORATION\n",
      "============================================================\n",
      "Dataset shape: (50000, 13)\n",
      "Memory usage: 4.96 MB\n",
      "\n",
      "Class Distribution:\n",
      "  Normal (0): 49,900 (99.80%)\n",
      "  Fraud (1): 100 (0.200%)\n",
      "  Imbalance ratio: 499:1\n",
      "\n",
      "Transaction Amount Analysis:\n",
      "Normal - Mean: $62.55, Median: $20.08\n",
      "Fraud - Mean: $807.03, Median: $41.24\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "============================================================\n",
      "DATA PREPROCESSING\n",
      "============================================================\n",
      "Feature engineering: 12 -> 24 features\n",
      "Data split: 40,000 train, 10,000 test\n",
      "Train fraud rate: 0.0020\n",
      "Test fraud rate: 0.0020\n",
      "‚úì Feature scaling completed\n",
      "‚úì Final feature count: 24\n",
      "\n",
      "============================================================\n",
      "HANDLING IMBALANCED DATA\n",
      "============================================================\n",
      "Original distribution: Counter({0.0: 39920, 1.0: 80})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\Aryan\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aryan\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aryan\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\Aryan\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE distribution: Counter({0.0: 39920, 1.0: 39920})\n",
      "Borderline SMOTE distribution: Counter({0.0: 39920, 1.0: 39920})\n",
      "Undersampled distribution: Counter({0.0: 240, 1.0: 80})\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING & EVALUATION\n",
      "============================================================\n",
      "\n",
      "--- Original Dataset (40,000 samples) ---\n",
      "  Logistic Regression: F1=0.950, Precision=0.950, Recall=0.950\n",
      "  Random Forest  : F1=0.889, Precision=1.000, Recall=0.800\n",
      "  Gradient Boosting: F1=0.872, Precision=0.895, Recall=0.850\n",
      "  SVM            : F1=0.889, Precision=1.000, Recall=0.800\n",
      "  Weighted LR    : F1=0.792, Precision=0.679, Recall=0.950\n",
      "  Weighted RF    : F1=0.889, Precision=1.000, Recall=0.800\n",
      "\n",
      "--- SMOTE Dataset (79,840 samples) ---\n",
      "  Logistic Regression: F1=0.826, Precision=0.731, Recall=0.950\n",
      "  Random Forest  : F1=0.895, Precision=0.944, Recall=0.850\n",
      "  Gradient Boosting: F1=0.826, Precision=0.731, Recall=0.950\n",
      "  SVM            : F1=0.633, Precision=0.475, Recall=0.950\n",
      "  Weighted LR    : F1=0.826, Precision=0.731, Recall=0.950\n",
      "  Weighted RF    : F1=0.895, Precision=0.944, Recall=0.850\n",
      "\n",
      "--- Borderline_SMOTE Dataset (79,840 samples) ---\n",
      "  Logistic Regression: F1=0.792, Precision=0.679, Recall=0.950\n",
      "  Random Forest  : F1=0.889, Precision=1.000, Recall=0.800\n",
      "  Gradient Boosting: F1=0.864, Precision=0.792, Recall=0.950\n",
      "  SVM            : F1=0.567, Precision=0.404, Recall=0.950\n",
      "  Weighted LR    : F1=0.792, Precision=0.679, Recall=0.950\n",
      "  Weighted RF    : F1=0.889, Precision=1.000, Recall=0.800\n",
      "\n",
      "--- Undersampled Dataset (320 samples) ---\n",
      "  Logistic Regression: F1=0.576, Precision=0.413, Recall=0.950\n",
      "  Random Forest  : F1=0.469, Precision=0.311, Recall=0.950\n",
      "  Gradient Boosting: F1=0.365, Precision=0.226, Recall=0.950\n",
      "  SVM            : F1=0.585, Precision=0.422, Recall=0.950\n",
      "  Weighted LR    : F1=0.442, Precision=0.288, Recall=0.950\n",
      "  Weighted RF    : F1=0.613, Precision=0.452, Recall=0.950\n",
      "\n",
      "============================================================\n",
      "RESULTS ANALYSIS\n",
      "============================================================\n",
      "üèÜ BEST MODELS BY METRIC:\n",
      "------------------------------------------------------------\n",
      "F1          : Logistic Regression on Original = 0.950\n",
      "Precision   : Random Forest on Original = 1.000\n",
      "Recall      : Logistic Regression on Original = 0.950\n",
      "ROC_AUC     : Random Forest on Original = 1.000\n",
      "Total_Cost  : Logistic Regression on Original = $5,050\n",
      "\n",
      "üìä TOP 5 MODELS BY F1-SCORE:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Logistic Regression  on Original       : P=0.950 R=0.950 F1=0.950 AUC=1.000 Cost=$5,050\n",
      "Random Forest        on SMOTE          : P=0.944 R=0.850 F1=0.895 AUC=0.974 Cost=$15,050\n",
      "Weighted RF          on SMOTE          : P=0.944 R=0.850 F1=0.895 AUC=0.974 Cost=$15,050\n",
      "Random Forest        on Original       : P=1.000 R=0.800 F1=0.889 AUC=1.000 Cost=$20,000\n",
      "SVM                  on Original       : P=1.000 R=0.800 F1=0.889 AUC=1.000 Cost=$20,000\n",
      "\n",
      "üìà DATASET PERFORMANCE COMPARISON:\n",
      "------------------------------------------------------------\n",
      "Original       : Avg F1=0.880, Max F1=0.950, Min Cost=$5,050\n",
      "SMOTE          : Avg F1=0.817, Max F1=0.895, Min Cost=$5,350\n",
      "Borderline_SMOTE: Avg F1=0.799, Max F1=0.889, Min Cost=$5,250\n",
      "Undersampled   : Avg F1=0.508, Max F1=0.613, Min Cost=$6,150\n",
      "\n",
      "üíº BUSINESS INSIGHTS:\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Best model: Logistic Regression on Original\n",
      "‚Ä¢ Fraud detection rate (Recall): 95.0%\n",
      "‚Ä¢ Precision (Accuracy when flagging fraud): 95.0%\n",
      "‚Ä¢ False positive rate: 0.01%\n",
      "‚Ä¢ Estimated daily cost: $5,050\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATIONS & NEXT STEPS\n",
      "============================================================\n",
      "üéØ DEPLOYMENT RECOMMENDATIONS:\n",
      "----------------------------------------\n",
      "1. Deploy: Logistic Regression trained on Original\n",
      "2. Expected performance: 95.0% fraud detection rate\n",
      "3. Monitor false positive rate: 0.01%\n",
      "4. Set up cost-sensitive thresholds for decision making\n",
      "\n",
      "üîß TECHNICAL IMPLEMENTATION:\n",
      "----------------------------------------\n",
      "1. Implement real-time scoring API\n",
      "2. Set up model monitoring and drift detection\n",
      "3. Create automated retraining pipeline\n",
      "4. Implement A/B testing framework\n",
      "5. Integrate with existing fraud systems\n",
      "\n",
      "üí° IMPROVEMENT OPPORTUNITIES:\n",
      "----------------------------------------\n",
      "1. Ensemble methods for better robustness\n",
      "2. Deep learning models (Neural Networks, Autoencoders)\n",
      "3. Advanced feature engineering (customer behavior patterns)\n",
      "4. Online learning for concept drift adaptation\n",
      "5. Integration with external data sources\n",
      "\n",
      "‚úÖ FRAUD DETECTION ANALYSIS COMPLETED!\n",
      "\n",
      "üöÄ Pipeline ready for production deployment!\n",
      "üìä Best F1-Score: 0.950\n",
      "üí∞ Lowest Cost: $5,050\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                           precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                           roc_curve, precision_recall_curve, auc, average_precision_score)\n",
    "\n",
    "# Import imbalanced-learn libraries\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "    from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "    print(\"‚úì imbalanced-learn library available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è imbalanced-learn not available. Install with: pip install imbalanced-learn\")\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "class FraudDetectionPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for credit card fraud detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = None\n",
    "        self.models = {}\n",
    "        self.results = []\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    def load_data(self, file_path=None):\n",
    "        \"\"\"\n",
    "        Load credit card fraud dataset\n",
    "        \n",
    "        For real data, download from:\n",
    "        https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "        \"\"\"\n",
    "        if file_path:\n",
    "            print(f\"Loading data from {file_path}...\")\n",
    "            data = pd.read_csv(file_path)\n",
    "            print(f\"Loaded {len(data):,} transactions\")\n",
    "        else:\n",
    "            print(\"Creating synthetic fraud detection dataset...\")\n",
    "            data = self._create_synthetic_dataset()\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _create_synthetic_dataset(self, n_samples=50000, fraud_rate=0.002):\n",
    "        \"\"\"Create realistic synthetic fraud dataset\"\"\"\n",
    "        n_fraud = int(n_samples * fraud_rate)\n",
    "        n_normal = n_samples - n_fraud\n",
    "        \n",
    "        print(f\"Generating {n_samples:,} transactions:\")\n",
    "        print(f\"  Normal: {n_normal:,} ({(1-fraud_rate)*100:.1f}%)\")\n",
    "        print(f\"  Fraud: {n_fraud:,} ({fraud_rate*100:.3f}%)\")\n",
    "        \n",
    "        # Normal transactions\n",
    "        normal_data = {\n",
    "            'Time': np.random.exponential(3600, n_normal),\n",
    "            'V1': np.random.normal(-0.3, 1.0, n_normal),\n",
    "            'V2': np.random.normal(0.1, 1.1, n_normal),\n",
    "            'V3': np.random.normal(-0.2, 1.2, n_normal),\n",
    "            'V4': np.random.normal(0.2, 1.0, n_normal),\n",
    "            'V5': np.random.normal(-0.1, 1.1, n_normal),\n",
    "            'V6': np.random.normal(0.0, 1.0, n_normal),\n",
    "            'V7': np.random.normal(-0.1, 1.0, n_normal),\n",
    "            'V8': np.random.normal(0.0, 1.1, n_normal),\n",
    "            'V9': np.random.normal(-0.2, 1.1, n_normal),\n",
    "            'V10': np.random.normal(-0.1, 1.0, n_normal),\n",
    "            'Amount': np.random.lognormal(3.0, 1.5, n_normal).clip(0, 10000),\n",
    "            'Class': np.zeros(n_normal)\n",
    "        }\n",
    "        \n",
    "        # Fraudulent transactions (different patterns)\n",
    "        fraud_data = {\n",
    "            'Time': np.random.exponential(1200, n_fraud),\n",
    "            'V1': np.random.normal(2.5, 1.2, n_fraud),\n",
    "            'V2': np.random.normal(-2.0, 1.3, n_fraud),\n",
    "            'V3': np.random.normal(1.8, 1.1, n_fraud),\n",
    "            'V4': np.random.normal(-2.2, 1.4, n_fraud),\n",
    "            'V5': np.random.normal(1.5, 1.0, n_fraud),\n",
    "            'V6': np.random.normal(-1.4, 1.2, n_fraud),\n",
    "            'V7': np.random.normal(1.0, 1.0, n_fraud),\n",
    "            'V8': np.random.normal(-1.1, 1.3, n_fraud),\n",
    "            'V9': np.random.normal(1.3, 1.1, n_fraud),\n",
    "            'V10': np.random.normal(-1.6, 1.2, n_fraud),\n",
    "            'Amount': np.random.lognormal(4.5, 2.5, n_fraud).clip(0, 25000),\n",
    "            'Class': np.ones(n_fraud)\n",
    "        }\n",
    "        \n",
    "        # Combine and shuffle data\n",
    "        all_data = {}\n",
    "        for key in normal_data.keys():\n",
    "            all_data[key] = np.concatenate([normal_data[key], fraud_data[key]])\n",
    "        \n",
    "        df = pd.DataFrame(all_data)\n",
    "        return df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def explore_data(self, data):\n",
    "        \"\"\"Comprehensive data exploration\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA EXPLORATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Dataset shape: {data.shape}\")\n",
    "        print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Class distribution\n",
    "        class_counts = data['Class'].value_counts()\n",
    "        fraud_rate = class_counts[1] / len(data)\n",
    "        \n",
    "        print(f\"\\nClass Distribution:\")\n",
    "        print(f\"  Normal (0): {class_counts[0]:,} ({class_counts[0]/len(data)*100:.2f}%)\")\n",
    "        print(f\"  Fraud (1): {class_counts[1]:,} ({fraud_rate*100:.3f}%)\")\n",
    "        print(f\"  Imbalance ratio: {class_counts[0]/class_counts[1]:.0f}:1\")\n",
    "        \n",
    "        # Statistical summary\n",
    "        print(f\"\\nTransaction Amount Analysis:\")\n",
    "        normal_amounts = data[data['Class'] == 0]['Amount']\n",
    "        fraud_amounts = data[data['Class'] == 1]['Amount']\n",
    "        \n",
    "        print(f\"Normal - Mean: ${normal_amounts.mean():.2f}, Median: ${normal_amounts.median():.2f}\")\n",
    "        print(f\"Fraud - Mean: ${fraud_amounts.mean():.2f}, Median: ${fraud_amounts.median():.2f}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = data.isnull().sum().sum()\n",
    "        print(f\"\\nMissing values: {missing}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"Data preprocessing with feature engineering\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA PREPROCESSING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = data.drop('Class', axis=1)\n",
    "        y = data['Class']\n",
    "        \n",
    "        # Feature engineering\n",
    "        X = self._feature_engineering(X)\n",
    "        \n",
    "        # Train-test split (stratified to maintain class balance)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
    "        print(f\"Train fraud rate: {y_train.mean():.4f}\")\n",
    "        print(f\"Test fraud rate: {y_test.mean():.4f}\")\n",
    "        \n",
    "        # Feature scaling with RobustScaler (less sensitive to outliers)\n",
    "        self.scaler = RobustScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "        \n",
    "        print(f\"‚úì Feature scaling completed\")\n",
    "        print(f\"‚úì Final feature count: {X.shape[1]}\")\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \n",
    "    def _feature_engineering(self, X):\n",
    "        \"\"\"Advanced feature engineering for fraud detection\"\"\"\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # PCA component statistics\n",
    "        pca_cols = [col for col in X.columns if col.startswith('V')]\n",
    "        if len(pca_cols) > 0:\n",
    "            X_new['PCA_sum'] = X[pca_cols].sum(axis=1)\n",
    "            X_new['PCA_std'] = X[pca_cols].std(axis=1)\n",
    "            X_new['PCA_max'] = X[pca_cols].max(axis=1)\n",
    "            X_new['PCA_min'] = X[pca_cols].min(axis=1)\n",
    "            X_new['PCA_range'] = X_new['PCA_max'] - X_new['PCA_min']\n",
    "        \n",
    "        # Time-based features\n",
    "        if 'Time' in X.columns:\n",
    "            X_new['Hour'] = (X['Time'] % 86400) // 3600\n",
    "            X_new['is_night'] = ((X_new['Hour'] >= 22) | (X_new['Hour'] <= 6)).astype(int)\n",
    "            X_new['is_weekend'] = ((X_new['Hour'] % 7) >= 5).astype(int)\n",
    "        \n",
    "        # Amount-based features\n",
    "        if 'Amount' in X.columns:\n",
    "            X_new['Amount_log'] = np.log1p(X['Amount'])\n",
    "            # Amount percentiles\n",
    "            percentiles = X['Amount'].quantile([0.25, 0.75, 0.9, 0.95, 0.99])\n",
    "            X_new['Amount_very_low'] = (X['Amount'] <= percentiles[0.25]).astype(int)\n",
    "            X_new['Amount_high'] = (X['Amount'] > percentiles[0.75]).astype(int)\n",
    "            X_new['Amount_very_high'] = (X['Amount'] > percentiles[0.95]).astype(int)\n",
    "        \n",
    "        print(f\"Feature engineering: {X.shape[1]} -> {X_new.shape[1]} features\")\n",
    "        return X_new\n",
    "    \n",
    "    def handle_imbalanced_data(self, X_train, y_train):\n",
    "        \"\"\"Apply techniques to handle imbalanced dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"HANDLING IMBALANCED DATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Original distribution: {Counter(y_train)}\")\n",
    "        \n",
    "        datasets = {'Original': (X_train, y_train)}\n",
    "        \n",
    "        # Apply SMOTE if available\n",
    "        if IMBLEARN_AVAILABLE:\n",
    "            smote = SMOTE(random_state=self.random_state, k_neighbors=5)\n",
    "            X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "            datasets['SMOTE'] = (X_smote, y_smote)\n",
    "            print(f\"SMOTE distribution: {Counter(y_smote)}\")\n",
    "            \n",
    "            # Borderline SMOTE (more conservative)\n",
    "            borderline_smote = BorderlineSMOTE(random_state=self.random_state)\n",
    "            X_borderline, y_borderline = borderline_smote.fit_resample(X_train, y_train)\n",
    "            datasets['Borderline_SMOTE'] = (X_borderline, y_borderline)\n",
    "            print(f\"Borderline SMOTE distribution: {Counter(y_borderline)}\")\n",
    "        \n",
    "        # Undersampling\n",
    "        X_under, y_under = self._random_undersample(X_train, y_train, ratio=3.0)\n",
    "        datasets['Undersampled'] = (X_under, y_under)\n",
    "        print(f\"Undersampled distribution: {Counter(y_under)}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def _random_undersample(self, X, y, ratio=2.0):\n",
    "        \"\"\"Random undersampling of majority class\"\"\"\n",
    "        minority_class = y.value_counts().idxmin()\n",
    "        majority_class = y.value_counts().idxmax()\n",
    "        \n",
    "        minority_indices = y[y == minority_class].index\n",
    "        majority_indices = y[y == majority_class].index\n",
    "        \n",
    "        n_minority = len(minority_indices)\n",
    "        n_majority_target = int(n_minority * ratio)\n",
    "        \n",
    "        # Random sample from majority class\n",
    "        majority_sampled = np.random.choice(\n",
    "            majority_indices, \n",
    "            min(n_majority_target, len(majority_indices)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        selected_indices = list(minority_indices) + list(majority_sampled)\n",
    "        return X.loc[selected_indices], y.loc[selected_indices]\n",
    "    \n",
    "    def train_models(self, datasets, X_test, y_test):\n",
    "        \"\"\"Train multiple models on different datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL TRAINING & EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Define models to test\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(random_state=self.random_state, n_estimators=100),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=self.random_state, n_estimators=100),\n",
    "            'SVM': SVC(random_state=self.random_state, probability=True)\n",
    "        }\n",
    "        \n",
    "        # Class-weighted versions for comparison\n",
    "        weighted_models = {\n",
    "            'Weighted LR': LogisticRegression(random_state=self.random_state, max_iter=1000, class_weight='balanced'),\n",
    "            'Weighted RF': RandomForestClassifier(random_state=self.random_state, n_estimators=100, class_weight='balanced'),\n",
    "        }\n",
    "        \n",
    "        all_models = {**models, **weighted_models}\n",
    "        results = []\n",
    "        \n",
    "        # Train models on each dataset\n",
    "        for dataset_name, (X_train, y_train) in datasets.items():\n",
    "            print(f\"\\n--- {dataset_name} Dataset ({len(X_train):,} samples) ---\")\n",
    "            \n",
    "            for model_name, model in all_models.items():\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Train model\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Predict on test set\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                    \n",
    "                    train_time = time.time() - start_time\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    metrics = self._evaluate_model(y_test, y_pred, y_pred_proba)\n",
    "                    \n",
    "                    result = {\n",
    "                        'Dataset': dataset_name,\n",
    "                        'Model': model_name,\n",
    "                        'Train_Time': train_time,\n",
    "                        **metrics\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"  {model_name:15s}: F1={metrics['F1']:.3f}, \"\n",
    "                          f\"Precision={metrics['Precision']:.3f}, \"\n",
    "                          f\"Recall={metrics['Recall']:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  {model_name:15s}: ERROR - {str(e)}\")\n",
    "        \n",
    "        self.results = pd.DataFrame(results)\n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_model(self, y_true, y_pred, y_pred_proba):\n",
    "        \"\"\"Comprehensive model evaluation for fraud detection\"\"\"\n",
    "        \n",
    "        # Basic classification metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        # ROC and PR AUC\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            roc_auc, pr_auc = 0.0, 0.0\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.size == 4:\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        \n",
    "        # Cost-sensitive analysis (fraud detection specific)\n",
    "        cost_fp = 50     # Cost of blocking legitimate transaction\n",
    "        cost_fn = 5000   # Cost of missing fraud (much higher impact)\n",
    "        total_cost = fp * cost_fp + fn * cost_fn\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f2_score = (5 * precision * recall) / (4 * precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'F2': f2_score,\n",
    "            'ROC_AUC': roc_auc,\n",
    "            'PR_AUC': pr_auc,\n",
    "            'Specificity': specificity,\n",
    "            'Total_Cost': total_cost,\n",
    "            'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn\n",
    "        }\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Comprehensive results analysis\"\"\"\n",
    "        if self.results.empty:\n",
    "            print(\"No results to analyze. Train models first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Best models by different metrics\n",
    "        metrics = ['F1', 'Precision', 'Recall', 'ROC_AUC', 'Total_Cost']\n",
    "        \n",
    "        print(\"üèÜ BEST MODELS BY METRIC:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric == 'Total_Cost':\n",
    "                best = self.results.loc[self.results[metric].idxmin()]\n",
    "                print(f\"{metric:12s}: {best['Model']} on {best['Dataset']} = ${best[metric]:,.0f}\")\n",
    "            else:\n",
    "                best = self.results.loc[self.results[metric].idxmax()]\n",
    "                print(f\"{metric:12s}: {best['Model']} on {best['Dataset']} = {best[metric]:.3f}\")\n",
    "        \n",
    "        # Top 5 models\n",
    "        print(\"\\nüìä TOP 5 MODELS BY F1-SCORE:\")\n",
    "        print(\"-\" * 100)\n",
    "        top5 = self.results.nlargest(5, 'F1')\n",
    "        cols = ['Model', 'Dataset', 'Precision', 'Recall', 'F1', 'ROC_AUC', 'Total_Cost']\n",
    "        for _, row in top5.iterrows():\n",
    "            print(f\"{row['Model']:20s} on {row['Dataset']:15s}: \"\n",
    "                  f\"P={row['Precision']:.3f} R={row['Recall']:.3f} \"\n",
    "                  f\"F1={row['F1']:.3f} AUC={row['ROC_AUC']:.3f} Cost=${row['Total_Cost']:,.0f}\")\n",
    "        \n",
    "        # Dataset comparison\n",
    "        print(\"\\nüìà DATASET PERFORMANCE COMPARISON:\")\n",
    "        print(\"-\" * 60)\n",
    "        dataset_perf = self.results.groupby('Dataset').agg({\n",
    "            'F1': ['mean', 'max'],\n",
    "            'Precision': ['mean', 'max'],\n",
    "            'Recall': ['mean', 'max'],\n",
    "            'Total_Cost': ['mean', 'min']\n",
    "        }).round(3)\n",
    "        \n",
    "        for dataset in self.results['Dataset'].unique():\n",
    "            subset = self.results[self.results['Dataset'] == dataset]\n",
    "            print(f\"{dataset:15s}: Avg F1={subset['F1'].mean():.3f}, \"\n",
    "                  f\"Max F1={subset['F1'].max():.3f}, \"\n",
    "                  f\"Min Cost=${subset['Total_Cost'].min():,.0f}\")\n",
    "        \n",
    "        # Business insights\n",
    "        best_model = self.results.loc[self.results['F1'].idxmax()]\n",
    "        print(\"\\nüíº BUSINESS INSIGHTS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"‚Ä¢ Best model: {best_model['Model']} on {best_model['Dataset']}\")\n",
    "        print(f\"‚Ä¢ Fraud detection rate (Recall): {best_model['Recall']:.1%}\")\n",
    "        print(f\"‚Ä¢ Precision (Accuracy when flagging fraud): {best_model['Precision']:.1%}\")\n",
    "        print(f\"‚Ä¢ False positive rate: {best_model['FP']/(best_model['FP']+best_model['TN']):.2%}\")\n",
    "        print(f\"‚Ä¢ Estimated daily cost: ${best_model['Total_Cost']:,.0f}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RECOMMENDATIONS & NEXT STEPS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        best = self.results.loc[self.results['F1'].idxmax()]\n",
    "        \n",
    "        print(\"üéØ DEPLOYMENT RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"1. Deploy: {best['Model']} trained on {best['Dataset']}\")\n",
    "        print(f\"2. Expected performance: {best['Recall']:.1%} fraud detection rate\")\n",
    "        print(f\"3. Monitor false positive rate: {best['FP']/(best['FP']+best['TN']):.2%}\")\n",
    "        print(f\"4. Set up cost-sensitive thresholds for decision making\")\n",
    "        \n",
    "        print(\"\\nüîß TECHNICAL IMPLEMENTATION:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"1. Implement real-time scoring API\")\n",
    "        print(\"2. Set up model monitoring and drift detection\")\n",
    "        print(\"3. Create automated retraining pipeline\")\n",
    "        print(\"4. Implement A/B testing framework\")\n",
    "        print(\"5. Integrate with existing fraud systems\")\n",
    "        \n",
    "        print(\"\\nüí° IMPROVEMENT OPPORTUNITIES:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"1. Ensemble methods for better robustness\")\n",
    "        print(\"2. Deep learning models (Neural Networks, Autoencoders)\")\n",
    "        print(\"3. Advanced feature engineering (customer behavior patterns)\")\n",
    "        print(\"4. Online learning for concept drift adaptation\")\n",
    "        print(\"5. Integration with external data sources\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete fraud detection pipeline\n",
    "    \"\"\"\n",
    "    print(\"üîí CREDIT CARD FRAUD DETECTION SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Technologies: Python, scikit-learn, imbalanced-learn\")\n",
    "    print(\"Dataset: Synthetic (replace with real creditcard.csv)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = FraudDetectionPipeline(random_state=42)\n",
    "    \n",
    "    # Load data (replace None with 'creditcard.csv' for real data)\n",
    "    data = pipeline.load_data(file_path=None)\n",
    "    \n",
    "    # Explore data\n",
    "    pipeline.explore_data(data)\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_test, y_train, y_test = pipeline.preprocess_data(data)\n",
    "    \n",
    "    # Handle imbalanced data\n",
    "    datasets = pipeline.handle_imbalanced_data(X_train, y_train)\n",
    "    \n",
    "    # Train models\n",
    "    results = pipeline.train_models(datasets, X_test, y_test)\n",
    "    \n",
    "    # Analyze results\n",
    "    pipeline.analyze_results()\n",
    "    \n",
    "    # Generate recommendations\n",
    "    pipeline.generate_recommendations()\n",
    "    \n",
    "    print(\"\\n‚úÖ FRAUD DETECTION ANALYSIS COMPLETED!\")\n",
    "    return pipeline\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete fraud detection pipeline\n",
    "    fraud_pipeline = main()\n",
    "    \n",
    "    print(f\"\\nüöÄ Pipeline ready for production deployment!\")\n",
    "    print(f\"üìä Best F1-Score: {fraud_pipeline.results['F1'].max():.3f}\")\n",
    "    print(f\"üí∞ Lowest Cost: ${fraud_pipeline.results['Total_Cost'].min():,.0f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d0d3e-198e-4dff-8321-51ff84af8235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
