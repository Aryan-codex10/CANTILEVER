{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf9a4e6-d0a1-4552-9001-afd797d4c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MOVIE SENTIMENT ANALYSIS PROJECT\n",
      "============================================================\n",
      "Creating sample dataset for demonstration...\n",
      "Created sample dataset with 2000 reviews\n",
      "\n",
      "Train set: 1600 reviews\n",
      "Test set: 400 reviews\n",
      "Comparing different models...\n",
      "--------------------------------------------------\n",
      "Training completed for Logistic with TFIDF\n",
      "Logistic with TFIDF       Accuracy: 1.0000\n",
      "Training completed for Svm with TFIDF\n",
      "Svm with TFIDF            Accuracy: 1.0000\n",
      "Training completed for Random_Forest with TFIDF\n",
      "Random_Forest with TFIDF  Accuracy: 1.0000\n",
      "Training completed for Naive_Bayes with COUNT\n",
      "Naive_Bayes with COUNT    Accuracy: 1.0000\n",
      "\n",
      "Best model: Logistic with TFIDF\n",
      "\n",
      "============================================================\n",
      "DETAILED EVALUATION OF BEST MODEL\n",
      "============================================================\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00       200\n",
      "    Positive       1.00      1.00      1.00       200\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "\n",
      "Most Positive Features:\n",
      " 1. storyline            ( 3.776)\n",
      " 2. superb               ( 2.080)\n",
      " 3. brilliant            ( 2.055)\n",
      " 4. excellent            ( 2.049)\n",
      " 5. outstanding          ( 2.030)\n",
      " 6. perfect              ( 2.010)\n",
      " 7. fantastic            ( 1.996)\n",
      " 8. incredible           ( 1.973)\n",
      " 9. magnificent          ( 1.969)\n",
      "10. amazing              ( 1.969)\n",
      "\n",
      "Most Negative Features:\n",
      " 1. plot                 (-3.788)\n",
      " 2. terrible             (-2.109)\n",
      " 3. horrible             (-2.094)\n",
      " 4. awful                (-2.060)\n",
      " 5. dreadful             (-2.035)\n",
      " 6. disappointing        (-2.028)\n",
      " 7. poor                 (-2.007)\n",
      " 8. waste                (-1.980)\n",
      " 9. boring               (-1.970)\n",
      "10. bad                  (-1.915)\n",
      "\n",
      "============================================================\n",
      "TESTING ON NEW REVIEWS\n",
      "============================================================\n",
      "\n",
      "Review 1: This movie was absolutely amazing! Great story and fantastic acting.\n",
      "Prediction: Positive (Confidence: 0.925)\n",
      "\n",
      "Review 2: Terrible film, waste of time. Very boring and poorly made.\n",
      "Prediction: Negative (Confidence: 0.970)\n",
      "\n",
      "Review 3: Outstanding cinematography and brilliant performances. Highly recommended!\n",
      "Prediction: Positive (Confidence: 0.948)\n",
      "\n",
      "Review 4: Worst movie ever! Complete garbage and total disappointment.\n",
      "Prediction: Negative (Confidence: 0.868)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Model ready for predictions!\n",
      "Use: best_model.predict_single('your review text')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text Processing Libraries\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    NLTK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"NLTK not available. Using basic preprocessing.\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class TextPreprocessorBasic:\n",
    "    \"\"\"Basic text preprocessing without external dependencies\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stop_words = {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "            'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "            'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "            'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after',\n",
    "            'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "            'further', 'then', 'once'\n",
    "        }\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing HTML, special characters, etc.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub('<.*?>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Keep only letters and spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "        return text.strip()\n",
    "\n",
    "    def tokenize_and_process(self, text, remove_stopwords=True):\n",
    "        \"\"\"Basic tokenization and processing\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        tokens = text.split()\n",
    "\n",
    "        if remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"Complete sentiment analysis pipeline\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessorBasic()\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        self.pipeline = None\n",
    "        self.model_name = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def create_pipeline(self, vectorizer_type='tfidf', model_type='logistic'):\n",
    "        \"\"\"Create a scikit-learn pipeline\"\"\"\n",
    "\n",
    "        # Choose vectorizer\n",
    "        if vectorizer_type == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english'\n",
    "            )\n",
    "        elif vectorizer_type == 'count':\n",
    "            vectorizer = CountVectorizer(\n",
    "                max_features=5000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english'\n",
    "            )\n",
    "\n",
    "        # Choose model\n",
    "        if model_type == 'logistic':\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        elif model_type == 'svm':\n",
    "            model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "        elif model_type == 'random_forest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        elif model_type == 'naive_bayes':\n",
    "            model = MultinomialNB()\n",
    "\n",
    "        # Create pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "        self.model_name = f\"{model_type.title()} with {vectorizer_type.upper()}\"\n",
    "        return self.pipeline\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Train the pipeline\"\"\"\n",
    "        # Preprocess training data\n",
    "        X_train_processed = X_train.apply(self.preprocessor.tokenize_and_process)\n",
    "\n",
    "        # Train pipeline\n",
    "        self.pipeline.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Store references\n",
    "        self.vectorizer = self.pipeline.named_steps['vectorizer']\n",
    "        self.model = self.pipeline.named_steps['classifier']\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "\n",
    "        print(f\"Training completed for {self.model_name}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_test_processed = X_test.apply(self.preprocessor.tokenize_and_process)\n",
    "\n",
    "        predictions = self.pipeline.predict(X_test_processed)\n",
    "        probabilities = self.pipeline.predict_proba(X_test_processed)\n",
    "\n",
    "        return predictions, probabilities\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict sentiment for a single text\"\"\"\n",
    "        processed_text = self.preprocessor.tokenize_and_process(text)\n",
    "        prediction = self.pipeline.predict([processed_text])[0]\n",
    "        probability = self.pipeline.predict_proba([processed_text])[0]\n",
    "\n",
    "        sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "        confidence = max(probability)\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'processed_text': processed_text,\n",
    "            'sentiment': sentiment,\n",
    "            'prediction': int(prediction),\n",
    "            'confidence': confidence,\n",
    "            'probabilities': {\n",
    "                'negative': probability[0],\n",
    "                'positive': probability[1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_feature_importance(self, top_n=10):\n",
    "        \"\"\"Get feature importance for interpretation\"\"\"\n",
    "        if hasattr(self.model, 'coef_'):\n",
    "            # For linear models\n",
    "            coefficients = self.model.coef_[0]\n",
    "            feature_importance = list(zip(self.feature_names, coefficients))\n",
    "            feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "            return {\n",
    "                'most_positive': [(feat, coef) for feat, coef in feature_importance if coef > 0][:top_n],\n",
    "                'most_negative': [(feat, coef) for feat, coef in feature_importance if coef < 0][:top_n]\n",
    "            }\n",
    "\n",
    "        elif hasattr(self.model, 'feature_importances_'):\n",
    "            # For tree-based models\n",
    "            importances = self.model.feature_importances_\n",
    "            feature_importance = list(zip(self.feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            return {'most_important': feature_importance[:top_n]}\n",
    "\n",
    "        return None\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        predictions, probabilities = self.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "        # Detailed metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'specificity': specificity,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': classification_report(y_test, predictions, target_names=['Negative', 'Positive'])\n",
    "        }\n",
    "\n",
    "def load_data(file_path=None):\n",
    "    \"\"\"Load IMDB dataset\"\"\"\n",
    "    if file_path:\n",
    "        # Load from file\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(data)} reviews from {file_path}\")\n",
    "    else:\n",
    "        # Create sample dataset for demonstration\n",
    "        print(\"Creating sample dataset for demonstration...\")\n",
    "\n",
    "        # Expanded sample dataset\n",
    "        positive_words = [\"excellent\", \"amazing\", \"brilliant\", \"outstanding\", \"fantastic\", \"superb\", \"wonderful\", \"incredible\", \"perfect\", \"magnificent\"]\n",
    "        negative_words = [\"terrible\", \"awful\", \"horrible\", \"dreadful\", \"disappointing\", \"boring\", \"worst\", \"bad\", \"poor\", \"waste\"]\n",
    "\n",
    "        reviews = []\n",
    "\n",
    "        # Create 1000 positive reviews\n",
    "        for i in range(1000):\n",
    "            review = f\"This movie is {np.random.choice(positive_words)} with {np.random.choice(positive_words)} acting and {np.random.choice(positive_words)} storyline.\"\n",
    "            reviews.append((review, \"positive\"))\n",
    "\n",
    "        # Create 1000 negative reviews  \n",
    "        for i in range(1000):\n",
    "            review = f\"This movie is {np.random.choice(negative_words)} with {np.random.choice(negative_words)} acting and {np.random.choice(negative_words)} plot.\"\n",
    "            reviews.append((review, \"negative\"))\n",
    "\n",
    "        data = pd.DataFrame(reviews, columns=['review', 'sentiment'])\n",
    "        print(f\"Created sample dataset with {len(data)} reviews\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def compare_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Compare multiple models and return the best one\"\"\"\n",
    "\n",
    "    models_to_test = [\n",
    "        ('tfidf', 'logistic'),\n",
    "        ('tfidf', 'svm'),\n",
    "        ('tfidf', 'random_forest'),\n",
    "        ('count', 'naive_bayes')\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    print(\"Comparing different models...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for vectorizer_type, model_type in models_to_test:\n",
    "        analyzer = SentimentAnalyzer()\n",
    "        analyzer.create_pipeline(vectorizer_type, model_type)\n",
    "        analyzer.train(X_train, y_train)\n",
    "\n",
    "        evaluation = analyzer.evaluate_model(X_test, y_test)\n",
    "        results[analyzer.model_name] = {\n",
    "            'analyzer': analyzer,\n",
    "            'evaluation': evaluation\n",
    "        }\n",
    "\n",
    "        print(f\"{analyzer.model_name:<25} Accuracy: {evaluation['accuracy']:.4f}\")\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['evaluation']['accuracy'])\n",
    "\n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    return results[best_model_name]['analyzer'], results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete sentiment analysis pipeline\"\"\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MOVIE SENTIMENT ANALYSIS PROJECT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load data\n",
    "    data = load_data()  # Creates sample data\n",
    "\n",
    "    # Prepare data\n",
    "    X = data['review']\n",
    "    y = (data['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain set: {len(X_train)} reviews\")\n",
    "    print(f\"Test set: {len(X_test)} reviews\")\n",
    "\n",
    "    # Compare models and get the best one\n",
    "    best_analyzer, all_results = compare_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Detailed evaluation of best model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETAILED EVALUATION OF BEST MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_evaluation = best_analyzer.evaluate_model(X_test, y_test)\n",
    "\n",
    "    print(f\"Accuracy: {best_evaluation['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {best_evaluation['precision']:.4f}\")\n",
    "    print(f\"Recall: {best_evaluation['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {best_evaluation['f1_score']:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(best_evaluation['classification_report'])\n",
    "\n",
    "    # Feature importance\n",
    "    importance = best_analyzer.get_feature_importance(top_n=10)\n",
    "    if importance:\n",
    "        if 'most_positive' in importance:\n",
    "            print(\"\\nMost Positive Features:\")\n",
    "            for i, (feature, coef) in enumerate(importance['most_positive'], 1):\n",
    "                print(f\"{i:2d}. {feature:20s} ({coef:6.3f})\")\n",
    "\n",
    "            print(\"\\nMost Negative Features:\")\n",
    "            for i, (feature, coef) in enumerate(importance['most_negative'], 1):\n",
    "                print(f\"{i:2d}. {feature:20s} ({coef:6.3f})\")\n",
    "\n",
    "    # Test on new reviews\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TESTING ON NEW REVIEWS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    test_reviews = [\n",
    "        \"This movie was absolutely amazing! Great story and fantastic acting.\",\n",
    "        \"Terrible film, waste of time. Very boring and poorly made.\",\n",
    "        \"Outstanding cinematography and brilliant performances. Highly recommended!\",\n",
    "        \"Worst movie ever! Complete garbage and total disappointment.\"\n",
    "    ]\n",
    "\n",
    "    for i, review in enumerate(test_reviews, 1):\n",
    "        result = best_analyzer.predict_single(review)\n",
    "        print(f\"\\nReview {i}: {review}\")\n",
    "        print(f\"Prediction: {result['sentiment']} (Confidence: {result['confidence']:.3f})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return best_analyzer\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete analysis\n",
    "    best_model = main()\n",
    "\n",
    "    # The trained model is now ready for use\n",
    "    print(\"\\nModel ready for predictions!\")\n",
    "    print(\"Use: best_model.predict_single('your review text')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a21b86-0011-4617-b8b5-12d4eb62bcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
